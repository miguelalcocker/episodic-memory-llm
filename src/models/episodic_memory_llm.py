# src/models/episodic_memory_llm.py
"""
EpisodicMemoryLLM - Tu modelo innovador que integra Temporal Knowledge Graphs
con LLMs para memoria epis√≥dica persistente
"""

import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
from typing import List, Dict, Tuple, Optional
import time
import logging
import sys
import os

# Importar TKG
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from memory.temporal_knowledge_graph import TemporalKnowledgeGraph

logger = logging.getLogger(__name__)

class EpisodicMemoryLLM(nn.Module):
    """
    LLM con memoria epis√≥dica basada en Temporal Knowledge Graphs
    TU INNOVACI√ìN PRINCIPAL
    """
    
    def __init__(
        self,
        model_name: str = "gpt2-medium",
        max_context_length: int = 1024,
        tkg_max_nodes: int = 5000,
        tkg_decay_rate: float = 0.1,
        device: str = None
    ):
        super().__init__()
        
        # Device setup
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        
        # Load base LLM
        logger.info(f"Loading {model_name} model...")
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.to(self.device)
        self.model.eval()
        
        # Initialize Temporal Knowledge Graph
        self.tkg = TemporalKnowledgeGraph(
            max_nodes=tkg_max_nodes,
            decay_rate=tkg_decay_rate
        )
        
        self.max_context_length = max_context_length
        self.conversation_history = []
        
        # Generation parameters optimized
        self.generation_config = {
            "temperature": 0.8,
            "top_p": 0.9,
            "do_sample": True,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 3
        }
        
        logger.info(f"EpisodicMemoryLLM initialized on {device}")
    
    def get_text_embedding(self, text: str) -> np.ndarray:
        """
        Obtener embedding del texto usando el LLM base
        """
        # Limpiar y truncar texto
        text = text.strip()[:500]  # L√≠mite de caracteres
        
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=256,
            truncation=True,
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.transformer(**inputs)
            # Mean pooling del √∫ltimo hidden state
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return embeddings.cpu().numpy()[0]
    
    def classify_content(self, text: str, role: str) -> str:
        """
        Clasificar el tipo de contenido - VERSI√ìN MEJORADA
        """
        text_lower = text.lower()
        
        if role == "user":
            # Informaci√≥n personal - PATRONES M√ÅS ESPEC√çFICOS
            if any(phrase in text_lower for phrase in ["my name is", "i'm", "i am", "i work as", "work at"]):
                return "personal_info"
            
            # Preferencias - DETECTAR MEJOR
            elif any(phrase in text_lower for phrase in ["i love", "i like", "i enjoy", "my favorite", "my hobby"]):
                return "preferences"
            
            # Eventos y experiencias - EXPANDIR
            elif any(phrase in text_lower for phrase in ["yesterday", "last week", "ago", "when i", "i went", "i visited"]):
                return "episodic"
            
            # Preguntas sobre memoria - M√ÅS PATRONES
            elif any(phrase in text_lower for phrase in ["remember", "what do you know", "tell me about", "what's my", "where do i"]):
                return "memory_query"
            
            # Preguntas contextuales - M√ÅS ESPEC√çFICO
            elif any(phrase in text_lower for phrase in ["recommend", "suggest", "should i", "what should", "can you"]):
                return "contextual_query"
            
            else:
                return "general"
        
        else:  # assistant
            return "response"
    
    def add_to_memory(self, text: str, role: str = "user", metadata: Dict = None):
        """
        A√±adir informaci√≥n al Temporal Knowledge Graph
        """
        # Clasificar contenido
        content_type = self.classify_content(text, role)
        
        # Crear metadata enriquecido
        enhanced_metadata = {
            "role": role,
            "content_type": content_type,
            "conversation_turn": len(self.conversation_history),
            "timestamp": time.time()
        }
        if metadata:
            enhanced_metadata.update(metadata)
        
        # Obtener embedding
        embedding = self.get_text_embedding(text)
        
        # A√±adir al TKG
        node_id = self.tkg.add_node(
            content=text,
            embedding=embedding,
            node_type=content_type,
            metadata=enhanced_metadata
        )
        
        # Actualizar historial de conversaci√≥n
        self.conversation_history.append({
            "role": role,
            "content": text,
            "node_id": node_id,
            "metadata": enhanced_metadata
        })
        
        logger.info(f"Added to TKG: {content_type} - {text[:50]}...")
        return node_id
    
    def retrieve_relevant_context(self, query: str, max_memories: int = 5) -> List[Dict]:
        """
        Recuperar contexto relevante - VERSI√ìN MEJORADA CON DEBUG
        """
        query_embedding = self.get_text_embedding(query)
        
        # B√∫squeda sem√°ntica en TKG
        relevant_nodes = self.tkg.search_by_content(
            query_embedding, 
            k=max_memories * 2,  # Buscar m√°s para filtrar mejor
            time_weight=0.2  # Menos peso temporal, m√°s sem√°ntico
        )
        
        print(f"DEBUG: TKG search found {len(relevant_nodes)} nodes")
        
        context_items = []
        for node_id, relevance_score in relevant_nodes:
            node = self.tkg.nodes_data[node_id]
            
            # Filtrar responses para evitar ruido
            if node.node_type == "response":
                continue
                
            context_item = {
                "content": node.content,
                "type": node.node_type,
                "relevance_score": relevance_score,
                "temporal_relevance": node.calculate_temporal_relevance(time.time()),
                "metadata": node.metadata
            }
            context_items.append(context_item)
        
        # Ordenar por relevance score
        context_items.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        print(f"DEBUG: Returning {len(context_items[:max_memories])} context items")
        return context_items[:max_memories]

    
    def build_episodic_context(self, user_input: str) -> str:
        """
        Construir contexto usando memoria epis√≥dica del TKG
        """
        # Recuperar memorias relevantes
        relevant_memories = self.retrieve_relevant_context(user_input, max_memories=4)
        
        context_parts = []
        
        # A√±adir memorias relevantes organizadas por tipo
        if relevant_memories:
            context_parts.append("Context from our previous conversations:")
            
            # Agrupar por tipo para mejor organizaci√≥n
            memory_by_type = {}
            for memory in relevant_memories:
                mem_type = memory["type"]
                if mem_type not in memory_by_type:
                    memory_by_type[mem_type] = []
                memory_by_type[mem_type].append(memory)
            
            # Ordenar tipos por importancia
            type_order = ["personal_info", "preferences", "episodic", "general", "response"]
            for mem_type in type_order:
                if mem_type in memory_by_type:
                    memories = memory_by_type[mem_type]
                    
                    if mem_type == "personal_info":
                        context_parts.append("Personal information I know about you:")
                    elif mem_type == "preferences":
                        context_parts.append("Your preferences and interests:")
                    elif mem_type == "episodic":
                        context_parts.append("Past experiences you've shared:")
                    
                    for memory in memories[:2]:  # M√°ximo 2 por tipo
                        context_parts.append(f"- {memory['content']}")
            
            context_parts.append("")
        
        # A√±adir conversaci√≥n reciente
        recent_turns = self.conversation_history[-4:]  # √öltimos 4 turnos
        if recent_turns:
            context_parts.append("Recent conversation:")
            for turn in recent_turns:
                role_label = "You" if turn["role"] == "user" else "Me"
                context_parts.append(f"{role_label}: {turn['content']}")
            context_parts.append("")
        
        # Prompt final
        context_parts.append(f"User: {user_input}")
        context_parts.append("Assistant: Based on what I know about you and our conversation, ")
        
        return "\n".join(context_parts)
    
    def generate_episodic_response(self, user_input: str, max_length: int = 50) -> str:
        """
        Generar respuesta usando memoria epis√≥dica - VERSI√ìN INTELIGENTE
        """
        # Determinar tipo de query
        query_type = self.classify_content(user_input, "user")
        
        # Para preguntas de memoria, usar retrieval directo
        if query_type in ["memory_query", "contextual_query"]:
            return self.generate_memory_based_response(user_input)
        
        # Para otros casos, usar generaci√≥n con contexto
        return self.generate_contextual_response(user_input, max_length)
    
    def generate_memory_based_response(self, user_input: str) -> str:
        """
        Generar respuesta directa basada en memoria - VERSI√ìN ARREGLADA
        """
        input_lower = user_input.lower()
        
        # Recuperar memorias relevantes
        memories = self.retrieve_relevant_context(user_input, max_memories=8)
        
        print(f"DEBUG: Found {len(memories)} memories for: {user_input}")
        for i, mem in enumerate(memories):
            print(f"  {i}: {mem['content'][:50]}... (type: {mem['type']}, score: {mem['relevance_score']:.3f})")
        
        # Extraer todos los contenidos de memoria para an√°lisis
        all_memory_content = " ".join([mem["content"].lower() for mem in memories])
        
        # JOB/WORK queries
        if any(word in input_lower for word in ["job", "work", "occupation", "profession"]):
            print("DEBUG: Processing job query")
            # Buscar patrones espec√≠ficos de trabajo
            job_patterns = [
                r"work as (?:a |an )?(\w+)",
                r"i'm (?:a |an )?(\w+)",
                r"i am (?:a |an )?(\w+)",
                r"job (?:as |is )?(?:a |an )?(\w+)"
            ]
            
            import re
            for memory in memories:
                content = memory["content"].lower()
                for pattern in job_patterns:
                    match = re.search(pattern, content)
                    if match:
                        job = match.group(1)
                        if job in ["teacher", "doctor", "engineer", "programmer", "developer", "software"]:
                            if "teacher" in content:
                                return "You work as a teacher."
                            elif "doctor" in content:
                                return "You work as a doctor."
                            elif "engineer" in content or "software" in content:
                                return "You work as a software engineer."
                            else:
                                return f"You work as a {job}."
            
            # B√∫squeda m√°s directa
            if "teacher" in all_memory_content:
                return "You work as a teacher."
            elif "software" in all_memory_content and "engineer" in all_memory_content:
                return "You work as a software engineer."
            elif "google" in all_memory_content and "engineer" in all_memory_content:
                return "You work as a software engineer at Google."
            elif "doctor" in all_memory_content:
                return "You work as a doctor."
                
            return "I don't have clear information about your specific job yet."
        
        # HOBBIES/INTERESTS queries
        elif any(word in input_lower for word in ["hobbies", "interests", "enjoy", "like", "love"]):
            print("DEBUG: Processing hobbies query")
            hobbies = []
            
            # Buscar hobbies espec√≠ficos
            hobby_keywords = {
                "reading": ["reading", "books", "novels"],
                "chess": ["chess"],
                "hiking": ["hiking", "outdoor"],
                "cooking": ["cooking", "cook"],
                "music": ["music", "singing"],
                "sports": ["sports", "football", "basketball"]
            }
            
            for hobby_name, keywords in hobby_keywords.items():
                for keyword in keywords:
                    if keyword in all_memory_content:
                        if hobby_name not in hobbies:
                            hobbies.append(hobby_name)
            
            if hobbies:
                if len(hobbies) == 1:
                    return f"You enjoy {hobbies[0]}."
                elif len(hobbies) == 2:
                    return f"You enjoy {hobbies[0]} and {hobbies[1]}."
                else:
                    return f"You enjoy {', '.join(hobbies[:-1])}, and {hobbies[-1]}."
            
            return "I don't have specific information about your hobbies yet."
        
        # NAME queries
        elif "name" in input_lower:
            print("DEBUG: Processing name query")
            # Buscar nombres en las memorias
            names = ["alice", "bob", "charlie", "diana", "emma", "frank"]
            for name in names:
                if name in all_memory_content:
                    return f"Your name is {name.capitalize()}."
            return "I don't remember your name yet."
        
        # LOCATION queries (where do I work, live, etc.)
        elif any(word in input_lower for word in ["where", "location"]):
            print("DEBUG: Processing location query")
            locations = ["google", "central high", "hospital", "university", "company"]
            for location in locations:
                if location in all_memory_content:
                    if "google" in location:
                        return "You work at Google."
                    elif "central high" in location:
                        return "You work at Central High School."
                    elif "hospital" in location:
                        return "You work at a hospital."
                    else:
                        return f"You mentioned {location}."
            return "I don't have specific location information yet."
        
        # RESTAURANT/FOOD queries
        elif any(word in input_lower for word in ["restaurant", "food", "eat", "dinner"]):
            print("DEBUG: Processing restaurant query")
            if "italian" in all_memory_content:
                response_parts = ["You went to an Italian restaurant"]
                if "carbonara" in all_memory_content:
                    response_parts.append("and loved the carbonara")
                if "incredible" in all_memory_content or "great" in all_memory_content:
                    response_parts.append("and thought the food was incredible")
                return ". ".join(response_parts) + "."
            return "I don't have information about restaurants you've visited."
        
        # RECOMMENDATION queries
        elif any(word in input_lower for word in ["recommend", "suggest", "activities"]):
            print("DEBUG: Processing recommendation query")
            interests = []
            
            # Recopilar intereses de las memorias
            if "hiking" in all_memory_content or "outdoor" in all_memory_content:
                interests.append("outdoor activities")
            if "reading" in all_memory_content:
                interests.append("reading")
            if "chess" in all_memory_content:
                interests.append("strategy games")
            if "san francisco" in all_memory_content:
                location = "San Francisco"
            else:
                location = None
            
            if interests and location:
                return f"Based on your interest in {', '.join(interests)}, I'd recommend outdoor activities in {location} like hiking in Golden Gate Park."
            elif interests:
                return f"Based on your interests in {', '.join(interests)}, I can suggest related activities."
            else:
                return "Tell me more about your interests so I can make better recommendations."
        
        # Fallback con algo de contexto
        if memories:
            return "I understand. Based on what you've told me, please feel free to share more."
        else:
            return "I understand. Could you tell me more about that?"

    
    def generate_contextual_response(self, user_input: str, max_length: int = 50) -> str:
        """
        Generar respuesta contextual para inputs generales
        """
        input_lower = user_input.lower()
        
        # Respuestas contextuales simples
        if any(greeting in input_lower for greeting in ["hi", "hello", "hey"]):
            if "alice" in input_lower:
                return "Hello Alice! Nice to meet you."
            else:
                return "Hello! I'm glad to chat with you."
        
        elif "thank" in input_lower:
            return "You're welcome! I'm here to help."
        
        elif any(word in input_lower for word in ["love", "enjoy", "like"]):
            return "That sounds interesting! Tell me more about what you enjoy."
        
        else:
            # Respuesta gen√©rica que acknowledges input
            return "I see. Please continue sharing with me."


    def clean_response_aggressive(self, response: str) -> str:
        """
        Limpieza agresiva de respuesta para evitar basura
        """
        # Eliminar cualquier cosa despu√©s de ciertos patrones
        cut_patterns = ["User:", "Assistant:", "http", "https", "(", ")", "[", "]"]
        for pattern in cut_patterns:
            if pattern in response:
                response = response.split(pattern)[0]
        
        # Eliminar l√≠neas que parecen basura
        lines = response.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            # Filtrar l√≠neas que parecen basura
            if (len(line) > 0 and 
                not line.startswith("(") and 
                not "http" in line.lower() and
                not "docs/" in line.lower() and
                len(line) < 200):  # L√≠neas muy largas suelen ser basura
                clean_lines.append(line)
        
        response = ' '.join(clean_lines)
        
        # Mantener solo primera oraci√≥n coherente
        sentences = response.split('.')
        if sentences:
            first_sentence = sentences[0].strip()
            if len(first_sentence) > 10:
                response = first_sentence + "."
            else:
                response = "I understand what you're telling me."
        
        # Fallback final
        if not response.strip() or len(response.strip()) < 5:
            response = "Thank you for sharing that with me."
        
        return response.strip()
    
    def chat(self, user_input: str) -> str:
        """
        Interfaz principal de chat con memoria epis√≥dica
        """
        logger.info(f"User input: {user_input}")
        
        # A√±adir input a memoria
        self.add_to_memory(user_input, role="user")
        
        # Generar respuesta usando memoria epis√≥dica
        response = self.generate_episodic_response(user_input)
        
        # A√±adir respuesta a memoria
        self.add_to_memory(response, role="assistant")
        
        logger.info(f"Assistant response: {response}")
        return response
    
    def get_memory_statistics(self) -> Dict:
        """
        Obtener estad√≠sticas de la memoria epis√≥dica
        """
        tkg_stats = self.tkg.get_statistics()
        
        stats = {
            "conversation_turns": len(self.conversation_history),
            "tkg_nodes": tkg_stats["total_nodes"],
            "tkg_edges": tkg_stats["total_edges"],
            "node_types": tkg_stats["node_types"],
            "temporal_span_hours": tkg_stats.get("temporal_span_hours", 0),
            "avg_node_strength": tkg_stats["avg_node_strength"],
            "avg_edge_strength": tkg_stats["avg_edge_strength"]
        }
        
        return stats
    
    def consolidate_memory(self):
        """
        Ejecutar consolidaci√≥n de memoria (simula sue√±o REM)
        """
        logger.info("Starting memory consolidation...")
        self.tkg.consolidate_memory()
        logger.info("Memory consolidation completed")
    
    def save_memory(self, filepath: str):
        """Guardar estado completo del modelo"""
        import json
        from pathlib import Path
        
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # Guardar TKG
        self.tkg.save(f"{filepath}_tkg.json")
        
        # Guardar historial de conversaci√≥n
        with open(f"{filepath}_conversation.json", 'w') as f:
            json.dump(self.conversation_history, f, indent=2)
        
        logger.info(f"EpisodicMemoryLLM saved to {filepath}")
    
    def load_memory(self, filepath: str):
        """Cargar estado del modelo"""
        import json
        
        # Cargar TKG
        self.tkg.load(f"{filepath}_tkg.json")
        
        # Cargar historial
        with open(f"{filepath}_conversation.json", 'r') as f:
            self.conversation_history = json.load(f)
        
        logger.info(f"EpisodicMemoryLLM loaded from {filepath}")


def test_episodic_memory_llm():
    """
    Test completo del modelo con memoria epis√≥dica
    """
    print("üß† Testing EpisodicMemoryLLM...")
    
    # Inicializar modelo
    model = EpisodicMemoryLLM(
        model_name="gpt2-medium",
        device="cpu",  # Cambiar a "cuda" si tienes GPU
        tkg_max_nodes=1000
    )
    
    # Test scenario que fall√≥ antes
    test_conversation = [
        "Hi, I'm Alice and I work as a teacher",
        "I love reading books in my free time", 
        "I also enjoy playing chess",
        "What do you know about my hobbies?",
        "What's my job?",
        "Can you recommend something based on my interests?"
    ]
    
    print("\n" + "="*60)
    print("EPISODIC MEMORY LLM TEST")
    print("="*60)
    
    for i, user_input in enumerate(test_conversation):
        print(f"\n--- Turn {i+1} ---")
        
        start_time = time.time()
        response = model.chat(user_input)
        response_time = time.time() - start_time
        
        print(f"User: {user_input}")
        print(f"Assistant: {response}")
        print(f"Response time: {response_time:.2f}s")
        
        # Mostrar estad√≠sticas cada 3 turnos
        if (i + 1) % 3 == 0:
            stats = model.get_memory_statistics()
            print(f"\nüìä Memory Stats:")
            print(f"  TKG Nodes: {stats['tkg_nodes']}")
            print(f"  TKG Edges: {stats['tkg_edges']}")
            print(f"  Node Types: {stats['node_types']}")
    
    # Test de memoria espec√≠fica
    print(f"\nüîç Testing specific memory retrieval:")
    memories = model.retrieve_relevant_context("hobbies", max_memories=3)
    for j, memory in enumerate(memories):
        print(f"  {j+1}. {memory['content']} (type: {memory['type']}, score: {memory['relevance_score']:.3f})")
    
    # Test de consolidaci√≥n
    print(f"\nüß† Testing memory consolidation...")
    model.consolidate_memory()
    
    final_stats = model.get_memory_statistics()
    print(f"Final TKG nodes: {final_stats['tkg_nodes']}")
    print(f"Final TKG edges: {final_stats['tkg_edges']}")
    
    # Guardar modelo
    print(f"\nüíæ Saving model...")
    model.save_memory("results/episodic_memory_test")
    
    print("\n‚úÖ EpisodicMemoryLLM test completed!")
    return model


def compare_with_baseline():
    """
    Comparaci√≥n directa con baseline para mostrar mejoras
    """
    print("\nü•ä BASELINE vs EPISODIC MEMORY LLM COMPARISON")
    print("="*60)
    
    # Test inputs que fallan en baseline
    test_inputs = [
        "Hi, I'm Alice and I work as a teacher",
        "What's my job?"
    ]
    
    # Test con EpisodicMemoryLLM
    print("\nüß† EpisodicMemoryLLM:")
    model = EpisodicMemoryLLM(device="cpu")
    
    for inp in test_inputs:
        response = model.chat(inp)
        print(f"Input: {inp}")
        print(f"Response: {response}")
        print()
    
    stats = model.get_memory_statistics()
    print(f"Memory nodes created: {stats['tkg_nodes']}")
    print(f"Node types: {stats['node_types']}")
    
    return model


if __name__ == "__main__":
    # Ejecutar test completo
    model = test_episodic_memory_llm()
    
    # Ejecutar comparaci√≥n
    # compare_with_baseline()
